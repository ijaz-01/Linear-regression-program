{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1345cb8a-b601-4b24-a90a-a53eba990e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe76b9d7a2d4a9cbb31f9901afc0bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0758269903258609\nModel uploaded successfully to Azure Blob Storage as linear_regression_model.pkl\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>prediction</th></tr></thead><tbody><tr><td>0.5873250063571279</td></tr><tr><td>0.5848573408881489</td></tr><tr><td>0.5464386244359438</td></tr><tr><td>0.530577604761266</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0.5873250063571279
        ],
        [
         0.5848573408881489
        ],
        [
         0.5464386244359438
        ],
        [
         0.530577604761266
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to Azure Blob Storage as predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook containing ETL, ML training, and deployment steps.\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import io\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL ML Training and Deployment\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: ETL Process (Extract, Transform, Load)\n",
    "\n",
    "# Example: Extracting data from an external CSV file or direct data input\n",
    "new_data = [\n",
    "    (1.5, 2.3, 3.1, 4.0, 5.2),\n",
    "    (1.4, 2.2, 3.0, 4.1, 5.1),\n",
    "    (1.7, 2.5, 3.3, 4.3, 5.4),\n",
    "    (1.8, 2.6, 3.4, 4.4, 5.5),\n",
    "]\n",
    "\n",
    "# Define the column names for the data\n",
    "columns = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n",
    "\n",
    "# Step 3: Load data into a PySpark DataFrame\n",
    "new_data_df = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "# Step 4: Feature Engineering (Transform Data)\n",
    "\n",
    "# Using VectorAssembler to combine features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(new_data_df)\n",
    "\n",
    "# Step 5: Machine Learning - Train a Model (Linear Regression)\n",
    "\n",
    "# For simplicity, we will use random data for training as an example\n",
    "X = np.random.rand(100, 5)  # Random features\n",
    "y = np.random.rand(100)  # Random target values\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# Step 6: Save the trained model to Azure Blob Storage\n",
    "\n",
    "# Azure Blob Storage details\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=ijazblob1;AccountKey=OHedg6WdUSruycWuXnyjZceDUXpNk3Mm8fBEFzQFq7mOzGyeH3JVSQ2tKCFwunaEaXhQkdT+7jYT+AStrnKF6Q==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"ijaz-container\"\n",
    "model_blob_name = \"linear_regression_model.pkl\"  # Model filename in the container\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get BlobClient to upload the model file\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=model_blob_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"/dbfs/tmp\", exist_ok=True)\n",
    "\n",
    "# Save the model as a .pkl file\n",
    "with open(\"/dbfs/tmp/linear_regression_model.pkl\", \"wb\") as model_file:\n",
    "    joblib.dump(model, model_file)\n",
    "\n",
    "# Upload the saved model file to Azure Blob Storage\n",
    "with open(\"/dbfs/tmp/linear_regression_model.pkl\", \"rb\") as model_file:\n",
    "    blob_client.upload_blob(model_file, overwrite=True)\n",
    "\n",
    "print(f\"Model uploaded successfully to Azure Blob Storage as {model_blob_name}\")\n",
    "\n",
    "# Step 7: Predicting with the trained model on new data\n",
    "\n",
    "# Generate predictions (in a real-world scenario, this would use new data)\n",
    "def generate_predictions(model, assembled_data):\n",
    "    # Convert assembled data to pandas and get features\n",
    "    pandas_df = assembled_data.select('features').toPandas()\n",
    "    features = np.array(pandas_df['features'].tolist())\n",
    "\n",
    "    # Use the trained model to predict\n",
    "    predictions = model.predict(features)\n",
    "\n",
    "    # Convert predictions back to a Spark DataFrame\n",
    "    prediction_df = spark.createDataFrame(pd.DataFrame({'prediction': predictions}))\n",
    "    return prediction_df\n",
    "\n",
    "# Generate predictions using the trained model\n",
    "prediction_df = generate_predictions(model, assembled_data)\n",
    "\n",
    "# Show predictions\n",
    "display(prediction_df)\n",
    "\n",
    "# Step 8: Save the predictions to Azure Blob Storage as a CSV file\n",
    "\n",
    "# Convert the Spark DataFrame to Pandas to write as CSV\n",
    "prediction_pandas_df = prediction_df.toPandas()\n",
    "\n",
    "# Save the predictions to a CSV file in Azure Blob Storage\n",
    "predictions_blob_name = \"predictions.csv\"\n",
    "predictions_blob_client = blob_service_client.get_blob_client(container=container_name, blob=predictions_blob_name)\n",
    "\n",
    "# Write the CSV to Blob Storage\n",
    "with io.BytesIO() as output:\n",
    "    prediction_pandas_df.to_csv(output, index=False)\n",
    "    output.seek(0)\n",
    "    predictions_blob_client.upload_blob(output, overwrite=True)\n",
    "\n",
    "print(f\"Predictions saved to Azure Blob Storage as {predictions_blob_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-12-07 23:24:50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}