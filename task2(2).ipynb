{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329835c8-e8f0-4495-8623-f6d71d15ada6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id     product_name  ... customer_location  target_column\n0        2001           Tablet  ...        New Jersey              1\n1        2002       Smartwatch  ...            Nevada              1\n2        2003        Microwave  ...          Illinois              0\n3        2004         Speakers  ...           Florida              1\n4        2005             Desk  ...           Arizona              0\n5        2006  Air Conditioner  ...             Texas              1\n6        2007        Projector  ...            Oregon              1\n7        2008       Dishwasher  ...              Ohio              0\n8        2009          Toaster  ...        California              1\n9        2010       VR Headset  ...          Michigan              1\n\n[10 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# Connection string to your Azure Blob Storage account\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=ijazblob1;AccountKey=OHedg6WdUSruycWuXnyjZceDUXpNk3Mm8fBEFzQFq7mOzGyeH3JVSQ2tKCFwunaEaXhQkdT+7jYT+AStrnKF6Q==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# Define the container name and the blob (file) name\n",
    "container_name = \"ijaz-container\"\n",
    "blob_name = \"data.csv\"\n",
    "\n",
    "# Initialize the BlobServiceClient using the connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get a BlobClient for the blob you want to read\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "# Download the blob's content into memory (as a stream)\n",
    "blob_data = blob_client.download_blob()\n",
    "\n",
    "# Read the content of the blob as a string (this will be CSV data)\n",
    "csv_data = blob_data.readall()\n",
    "\n",
    "# Convert the CSV data into a pandas DataFrame for easy manipulation\n",
    "data_frame = pd.read_csv(io.BytesIO(csv_data))\n",
    "\n",
    "# Print the DataFrame to see the contents\n",
    "print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7a842f-20de-41cf-b3f9-ff7154216373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id  price  ...  price_quantity_interaction  price_per_unit\n0        2001    500  ...                       10000       23.809524\n1        2002    250  ...                        8750        6.944444\n2        2003    150  ...                        2250        9.375000\n3        2004    120  ...                        2160        6.315789\n4        2005    180  ...                        2160       13.846154\n5        2006    700  ...                        6300       70.000000\n6        2007    300  ...                        8400       10.344828\n7        2008    800  ...                        8000       72.727273\n8        2009     60  ...                        1500        2.307692\n9        2010    350  ...                        5600       20.588235\n\n[10 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming data_frame is your cleaned DataFrame\n",
    "\n",
    "# 1. Cumulative Sum for 'quantity_sold'\n",
    "data_frame['cumulative_quantity_sold'] = data_frame['quantity_sold'].cumsum()\n",
    "\n",
    "# 2. Rolling Averages\n",
    "# We can calculate rolling averages for numerical columns like 'price', 'quantity_sold', and 'discount'\n",
    "# For simplicity, we use a 3-row rolling window for each of these columns\n",
    "\n",
    "data_frame['rolling_avg_price'] = data_frame['price'].rolling(window=3).mean()\n",
    "data_frame['rolling_avg_quantity_sold'] = data_frame['quantity_sold'].rolling(window=3).mean()\n",
    "data_frame['rolling_avg_discount'] = data_frame['discount'].rolling(window=3).mean()\n",
    "\n",
    "# 3. Date-based Features (if you have a 'date' column)\n",
    "# For this example, assume the date column exists and is named 'date'\n",
    "# You can extract year, month, day, weekday, etc.\n",
    "# If there's no date column, you can skip this step\n",
    "\n",
    "# Example: Assuming 'date' is a datetime column\n",
    "if 'date' in data_frame.columns:\n",
    "    data_frame['year'] = data_frame['date'].dt.year\n",
    "    data_frame['month'] = data_frame['date'].dt.month\n",
    "    data_frame['day'] = data_frame['date'].dt.day\n",
    "    data_frame['weekday'] = data_frame['date'].dt.weekday\n",
    "\n",
    "# 4. One-Hot Encoding for 'category' and 'customer_location'\n",
    "# One-Hot Encoding using pandas' get_dummies\n",
    "if 'category' in data_frame.columns and 'customer_location' in data_frame.columns:\n",
    "    data_frame = pd.get_dummies(data_frame, columns=['category', 'customer_location'], drop_first=True)\n",
    "\n",
    "# 5. Creating Interaction Features\n",
    "# For example, the interaction between 'price' and 'quantity_sold' might be useful\n",
    "data_frame['price_quantity_interaction'] = data_frame['price'] * data_frame['quantity_sold']\n",
    "\n",
    "# 6. Feature Transformation: Log Transformation (for highly skewed features)\n",
    "# Apply log transformation to 'price' if it is highly skewed\n",
    "if data_frame['price'].skew() > 1:  # check if skew is significant\n",
    "    data_frame['log_price'] = np.log1p(data_frame['price'])  # log1p handles 0 values safely\n",
    "\n",
    "# Apply log transformation to 'quantity_sold' if needed\n",
    "if data_frame['quantity_sold'].skew() > 1:\n",
    "    data_frame['log_quantity_sold'] = np.log1p(data_frame['quantity_sold'])\n",
    "\n",
    "# 7. Additional Custom Feature (e.g., price per unit sold)\n",
    "data_frame['price_per_unit'] = data_frame['price'] / (data_frame['quantity_sold'] + 1)\n",
    "\n",
    "# 8. Remove any unnecessary columns (optional)\n",
    "# Example: If 'product_name' and 'target_column' aren't useful for modeling, drop them\n",
    "columns_to_drop = ['product_name', 'target_column']\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in data_frame.columns]\n",
    "data_frame = data_frame.drop(columns=existing_columns_to_drop)\n",
    "\n",
    "# Show the final engineered DataFrame\n",
    "print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37cc0436-8620-48e2-aa20-1c8d43fdd263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id     product_name  ... customer_location  target_column\n0        2001           Tablet  ...        New Jersey              1\n1        2002       Smartwatch  ...            Nevada              1\n2        2003        Microwave  ...          Illinois              0\n3        2004         Speakers  ...           Florida              1\n4        2005             Desk  ...           Arizona              0\n5        2006  Air Conditioner  ...             Texas              1\n6        2007        Projector  ...            Oregon              1\n7        2008       Dishwasher  ...              Ohio              0\n8        2009          Toaster  ...        California              1\n9        2010       VR Headset  ...          Michigan              1\n\n[10 rows x 9 columns]\nThe 'target_column' does not exist in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Connection string to your Azure Blob Storage account\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=ijazblob1;AccountKey=OHedg6WdUSruycWuXnyjZceDUXpNk3Mm8fBEFzQFq7mOzGyeH3JVSQ2tKCFwunaEaXhQkdT+7jYT+AStrnKF6Q==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# Define the container name and the blob (file) name\n",
    "container_name = \"ijaz-container\"\n",
    "blob_name = \"data.csv\"\n",
    "\n",
    "# Initialize the BlobServiceClient using the connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get a BlobClient for the blob you want to read\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "# Download the blob's content into memory (as a stream)\n",
    "blob_data = blob_client.download_blob()\n",
    "\n",
    "# Read the content of the blob as a string (this will be CSV data)\n",
    "csv_data = blob_data.readall()\n",
    "\n",
    "# Convert the CSV data into a pandas DataFrame for easy manipulation\n",
    "data_frame = pd.read_csv(io.BytesIO(csv_data))\n",
    "\n",
    "# Print the DataFrame to see the contents\n",
    "print(data_frame)\n",
    "\n",
    "# Assuming data_frame is your cleaned DataFrame\n",
    "\n",
    "# 1. Cumulative Sum for 'quantity_sold'\n",
    "data_frame['cumulative_quantity_sold'] = data_frame['quantity_sold'].cumsum()\n",
    "\n",
    "# 2. Rolling Averages\n",
    "data_frame['rolling_avg_price'] = data_frame['price'].rolling(window=3).mean()\n",
    "data_frame['rolling_avg_quantity_sold'] = data_frame['quantity_sold'].rolling(window=3).mean()\n",
    "data_frame['rolling_avg_discount'] = data_frame['discount'].rolling(window=3).mean()\n",
    "\n",
    "# 3. One-Hot Encoding for 'category' and 'customer_location'\n",
    "data_frame = pd.get_dummies(data_frame, columns=['category', 'customer_location'], drop_first=True)\n",
    "\n",
    "# 4. Creating Interaction Features\n",
    "data_frame['price_quantity_interaction'] = data_frame['price'] * data_frame['quantity_sold']\n",
    "\n",
    "# 5. Feature Transformation: Log Transformation (for highly skewed features)\n",
    "if data_frame['price'].skew() > 1:\n",
    "    data_frame['log_price'] = np.log1p(data_frame['price'])\n",
    "\n",
    "if data_frame['quantity_sold'].skew() > 1:\n",
    "    data_frame['log_quantity_sold'] = np.log1p(data_frame['quantity_sold'])\n",
    "\n",
    "# 6. Additional Custom Feature (e.g., price per unit sold)\n",
    "data_frame['price_per_unit'] = data_frame['price'] / (data_frame['quantity_sold'] + 1)\n",
    "\n",
    "# 7. Remove any unnecessary columns if they exist\n",
    "columns_to_drop = ['product_name', 'target_column']\n",
    "data_frame = data_frame.drop(columns=[col for col in columns_to_drop if col in data_frame.columns])\n",
    "\n",
    "# Define X (features) and y (target variable) if 'target_column' exists\n",
    "if 'target_column' in data_frame.columns:\n",
    "    X = data_frame.drop(columns=['target_column'])  # Features\n",
    "    y = data_frame['target_column']  # Target variable\n",
    "\n",
    "    # Drop rows with NaN values in the feature matrix\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]  # Ensure that y matches the rows in X\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Show the final engineered DataFrame\n",
    "    print(X_train)\n",
    "else:\n",
    "    print(\"The 'target_column' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16022a3-2c96-42cd-a7e2-ea93ae16c2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc02c72b8f04ff881dada71398cb72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.3333333333333339\nMean Squared Error (MSE): 0.11111111111111151\nRoot Mean Squared Error (RMSE): 0.3333333333333339\nR-squared (R²): nan\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define X and y\n",
    "X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n",
    "y = np.array([1, 2, 3])\n",
    "\n",
    "# Impute missing values in X\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets with a smaller test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R²): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6536888-fc43-423e-9111-8077355a2d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004af2ed25c443d492277c5142dd8a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.3333333333333339\nMean Squared Error (MSE): 0.11111111111111151\nRoot Mean Squared Error (RMSE): 0.3333333333333339\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R²): nan\nCross-Validation RMSE Scores: [0.33333333 0.15384615 0.2       ]\nMean CV RMSE: 0.229059829059829\nCross-Validation R² Scores: [nan nan nan]\nMean CV R²: nan\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n/databricks/python/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n/databricks/python/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1187: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'X' is the feature matrix and 'y' is the target variable\n",
    "# Impute missing values in X\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using RMSE and R² on the test set\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "\n",
    "# Cross-validation evaluation using RMSE and R²\n",
    "kf = KFold(n_splits=min(5, len(X_imputed)), shuffle=True, random_state=42)\n",
    "\n",
    "# Using built-in scoring methods for cross-validation\n",
    "cv_rmse_scores = cross_val_score(model, X_imputed, y, cv=kf, scoring='neg_root_mean_squared_error')\n",
    "cv_r2_scores = cross_val_score(model, X_imputed, y, cv=kf, scoring='r2')\n",
    "\n",
    "# Convert negative RMSE to positive values\n",
    "cv_rmse_scores = -cv_rmse_scores\n",
    "\n",
    "print(f\"Cross-Validation RMSE Scores: {cv_rmse_scores}\")\n",
    "print(f\"Mean CV RMSE: {cv_rmse_scores.mean()}\")\n",
    "\n",
    "print(f\"Cross-Validation R² Scores: {cv_r2_scores}\")\n",
    "print(f\"Mean CV R²: {cv_r2_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6d188a-3d3e-41d4-8c68-23140574e442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9e19de7437414bb98d0adef804a916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.10014911552688335\nModel successfully uploaded to Azure Blob Storage as linear_regression_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming 'X' is the feature matrix and 'y' is the target variable\n",
    "# Example: Train a linear regression model (replace with your model and training logic)\n",
    "# Here, I use random data as an example. Replace this with your actual dataset.\n",
    "X = np.random.rand(100, 5)  # 100 samples, 5 features\n",
    "y = np.random.rand(100)  # 100 target values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# Save the trained model to a local file\n",
    "model_filename = \"linear_regression_model.pkl\"\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "# Azure Blob Storage details\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=ijazblob1;AccountKey=OHedg6WdUSruycWuXnyjZceDUXpNk3Mm8fBEFzQFq7mOzGyeH3JVSQ2tKCFwunaEaXhQkdT+7jYT+AStrnKF6Q==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"ijaz-container\"\n",
    "model_blob_name = \"linear_regression_model.pkl\"  # Blob name in the container\n",
    "\n",
    "# Initialize the BlobServiceClient using the connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get the BlobClient for the model file\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=model_blob_name)\n",
    "\n",
    "# Upload the model file to Azure Blob Storage\n",
    "try:\n",
    "    # Open the model file in binary mode and upload to Blob Storage\n",
    "    with open(model_filename, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)  # overwrite=True to replace any existing file\n",
    "\n",
    "    print(f\"Model successfully uploaded to Azure Blob Storage as {model_blob_name}\")\n",
    "\n",
    "    # Optionally, remove the local model file after uploading\n",
    "    os.remove(model_filename)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading model to Azure Blob Storage: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-12-07 22:37:23",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}